{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import string\n",
    "import dateutil.parser as parser\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('TED_Talks_by_ID_plus-transcripts-and-LIWC-and-MFT-plus-views.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count words after stripping times and pauses\n",
    "df['text'] = df.transcript.map(lambda x: re.sub(r'[0-9]+:[0-9]+', '', str(x)))\n",
    "\n",
    "stopwords = ['(Laughter)', '(Applause)']\n",
    "for i in stopwords:\n",
    "        df['text'] = df.text.map(lambda x: x.replace(i, ''))\n",
    "        \n",
    "df['words_n'] = df.text.map(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exclude those with no text\n",
    "df = df[df['words_n'] > 1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 2 'clean' versions - 'transcript' preserves laughter/applause\n",
    "df['text_clean'] = df.text.map(lambda x: re.sub(r'[\\r]+', ' ', x)).map(lambda x: ' '.join(x.split()))\n",
    "df['transcript_clean'] = df.transcript.map(lambda x: re.sub(r'[\\r]+', ' ', str(x))).map(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text_clean[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary'] = None\n",
    "\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        df['summary'][i] = summarize(df.text_clean[i], ratio = 0.05, word_count=None, split=False)\n",
    "    except:\n",
    "        df['summary'][i] = 'ERROR'\n",
    "\n",
    "df['summary'] = df.summary.map(lambda x: re.sub(r'[\\n]+', ' ', x))\n",
    "df = df[df.summary != 'ERROR' ].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New variables to create:  \n",
    "* Categories of the most common topics\n",
    "* Log-transform views\n",
    "* Length (time)\n",
    "* Speed of talking\n",
    "* Laughter (n, rate)\n",
    "* Applause (n)\n",
    "* Questions (n)\n",
    "* Stories (n)\n",
    "* Exclamation (n)\n",
    "* Year and season\n",
    "* References to people\n",
    "* Filler words - e.g., 'so', 'um'\n",
    "* % of nouns, verbs, adjectives  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create categories of tags - exclude TED tags\n",
    "df['tags_list'] = df.tags.map(lambda x: re.findall(r\"[\\w']+\", x))\n",
    "\n",
    "master = []\n",
    "for i in range(len(df)):\n",
    "    master.extend(df.tags_list[i])\n",
    "\n",
    "ignore = ['TED', 'TEDx']\n",
    "tags = collections.Counter(x for x in master if x not in ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to most common 20\n",
    "tags_common = [x[0] for x in tags.most_common(20)]\n",
    "tags_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in tags_common:\n",
    "    df[i] = df['tags_list'].apply(lambda x: 1 if i in str(x) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Log-transform views\n",
    "df['views'] = df['views_as_of_06162017']\n",
    "df['log_views'] = df.views.map(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specific components\n",
    "df['laughter_n'] = df.transcript_clean.apply(lambda x: str(x).count('(Laughter)'))\n",
    "df['applause_n'] = df.transcript_clean.apply(lambda x: str(x).count('(Applause)'))\n",
    "df['questions_n'] = df.transcript_clean.apply(lambda x: str(x).count('?'))\n",
    "df['stories_n'] = df.transcript_clean.apply(lambda x: str(x).count('story' or 'stories'))\n",
    "df['exclamation_n'] = df.transcript_clean.apply(lambda x: str(x).count('!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert duration to float\n",
    "df['temp'] = df.duration.map(lambda x: x.split(':'))\n",
    "df['time'] = df.temp.map(lambda x: int(x[0])*60 + int(x[1]) + int(x[2])/60)\n",
    "del df['temp']\n",
    "\n",
    "# Rate of talking and laughter\n",
    "df['talking_speed'] = df.words_n / df.time\n",
    "df['laughter_speed'] = df.laughter_n / df.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sentences - n and length\n",
    "df['sentences_n'] = df.text_clean.map(lambda x: len(sent_tokenize(x)))\n",
    "df['sentence_length'] = df.words_n / df.sentences_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Year and season\n",
    "df['year'] = df.date_published.map(lambda x: parser.parse(x).year)\n",
    "df['month'] = df.date_published.map(lambda x: parser.parse(x).month)\n",
    "\n",
    "seasons = {\"season\": {1: 'Winter', 2: 'Winter', 3: 'Spring',\n",
    "                     4: 'Spring', 5: 'Spring', 6: 'Summer', 7: 'Summer', \n",
    "                     8: 'Summer', 9: 'Fall', 10: 'Fall', \n",
    "                     11: 'Fall', 12: 'Winter' }}\n",
    "\n",
    "df['season'] = df.month\n",
    "df.replace(seasons, inplace=True)\n",
    "\n",
    "seasons = pd.get_dummies(df['season'])\n",
    "df = pd.concat([df, seasons], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# References to people\n",
    "df['he'] = df.transcript_clean.apply(lambda x: str(x).lower().count(' he '))\n",
    "df['she'] = df.transcript_clean.apply(lambda x: str(x).lower().count(' she '))\n",
    "df['he_she'] = df.he + df.she\n",
    "df['self'] = df.transcript_clean.apply(lambda x: str(x).count(' I '))\n",
    "df['we'] = df.transcript_clean.apply(lambda x: str(x).lower().count(' we '))\n",
    "df['you'] = df.transcript_clean.apply(lambda x: str(x).lower().count(' you '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['filler'] = df.transcript_clean.apply(lambda x: sum(str(x).lower().count(i) for i in (\" um \", \" uh \", \" so \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filler.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parts of speech\n",
    "from collections import Counter\n",
    "df['counts'] = None\n",
    "\n",
    "for i in range(len(df)):\n",
    "    words = word_tokenize(df.transcript[i])\n",
    "    df['counts'][i] = Counter(tag for word,tag in pos_tag(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adjectives = ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']\n",
    "nouns = ['NN', 'NNP', 'NNPS', 'NNS']\n",
    "verbs = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "df['adjectives'] = df.counts.apply(lambda x: sum((x[a] for a in adjectives)))\n",
    "df['verbs'] = df.counts.apply(lambda x: sum((x[a] for a in verbs)))\n",
    "df['nouns'] = df.counts.apply(lambda x: sum((x[a] for a in nouns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in ['adjectives', 'verbs', 'nouns']:\n",
    "    name = i + '_percent'\n",
    "    df[name] = df[i] / df.words_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to use for mapping sentiment change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a moving window to map sentiment\n",
    "def window(seq, overlap):  \n",
    "    for pos in range(0, len(seq), 1):\n",
    "        yield seq[pos : pos + overlap]\n",
    "\n",
    "def merge(seq, slide):\n",
    "    for pos in range(0, len(seq), slide):\n",
    "        yield seq[pos : pos + slide] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Graph sentiment over time using a sliding window of text\n",
    "def storyarc(i, overlap, slide):\n",
    "    global d \n",
    "    d = {} \n",
    "\n",
    "    delim = \" \"\n",
    "    words = [s for s in df.text[i].split()] \n",
    "    merged_words = [' '.join(w) for w in merge(words, slide)]\n",
    "\n",
    "    delim = \" \"\n",
    "    samples = [delim.join(s) for s in window(merged_words, overlap)] \n",
    "    d['samples'] = samples\n",
    "\n",
    "    # Score sentiment\n",
    "\n",
    "    sentiments = [TextBlob(x).sentiment.polarity for x in samples]\n",
    "    d['scores'] = sentiments\n",
    "    \n",
    "    test.append(sentiments)\n",
    "    df['sentiment_array'][i] = np.asarray(sentiments)\n",
    "    df['sentiment_array_n'][i] = len(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Graph sentiment shape for a particular speaker\n",
    "def sentiment_graph(i, degrees):\n",
    "    sentiments = df.sentiment_array_interp[i]\n",
    "    x = range(len(sentiments))\n",
    "    y = sentiments\n",
    "    z = np.polyfit(x, y, degrees)\n",
    "    y_new = np.polyval(z,x)\n",
    "\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([-1,1])\n",
    "    \n",
    "    plt.suptitle('Story Arc', fontsize=16, fontname = \"Helvetica\")\n",
    "    plt.xlabel('Text window', fontsize=14, fontname = \"Helvetica\")\n",
    "    plt.ylabel('Sentiment', fontsize=14, fontname = \"Helvetica\")\n",
    "    plt.plot(x,y,'b-')\n",
    "    plt.plot(x,y_new,'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis - overall and over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sentiment - overall\n",
    "df['polarity'] = df.text_clean.map(lambda x: TextBlob(x).sentiment.polarity)\n",
    "df['subjectivity'] = df.text_clean.map(lambda x: TextBlob(x).sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Array of sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = []\n",
    "\n",
    "df['sentiment_array'] = None\n",
    "df['sentiment_array_n'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    storyarc(i, 2, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['min_sentiment'] = df.sentiment_array.map(lambda x: min(x))\n",
    "df['max_sentiment'] = df.sentiment_array.map(lambda x: max(x))\n",
    "df['sentiment_range'] = df.max_sentiment - df.min_sentiment \n",
    "df['sentiment_std'] = df.sentiment_array.map(lambda x: np.std(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering of sentiment shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use k-Shape algorithm to identify shape patterns in sentiment time series generated above. Algorithm will be used to identify \"story arc\" clusters.\n",
    "\n",
    "More information on k-Shape is available here: http://www.cs.columbia.edu/~jopa/kshape.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages\")\n",
    "from kshape.core import kshape, zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_array = df.sentiment_array_n.median()\n",
    "print(med_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress and stretch arrays to equal same size\n",
    "import scipy.interpolate as interp\n",
    "\n",
    "df['sentiment_array_interp'] = None\n",
    "\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        if df['sentiment_array_n'][i] <= med_array:\n",
    "            arr_interp = interp.interp1d(np.arange(df['sentiment_array'][i].size), df['sentiment_array'][i])\n",
    "            arr_stretch = arr_interp(np.linspace(0, df['sentiment_array'][i].size-1, 83))\n",
    "            df['sentiment_array_interp'][i] = arr_stretch\n",
    "        elif df['sentiment_array_n'][i] > med_array:\n",
    "            arr_interp = interp.interp1d(np.arange(df['sentiment_array'][i].size), df['sentiment_array'][i])\n",
    "            arr_compress = arr_interp(np.linspace(0, df['sentiment_array'][i].size-1, 83))\n",
    "            df['sentiment_array_interp'][i] = arr_compress\n",
    "    except:\n",
    "        df['sentiment_array_interp'][i] = 'ERROR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.sentiment_array_interp != 'ERROR'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arrays = []\n",
    "for i in range(len(df)):\n",
    "    arrays.append(df.sentiment_array_interp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_num = 6\n",
    "clusters = kshape(zscore(arrays, axis=1), cluster_num)\n",
    "for i in range(cluster_num):\n",
    "    print(len(clusters[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters[0][1][120:130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_graph(0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_groups = [x[1] for x in clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['index'] = df.index\n",
    "\n",
    "for i in range(len(cluster_groups)):\n",
    "    df['cluster_num' + str(i)] = df['index'].apply(lambda x: 1 if x in cluster_groups[i] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXPORT DATA\n",
    "file = 'df_text_122417'\n",
    "fileobj = open(file,'wb') \n",
    "pickle.dump(df,fileobj) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
