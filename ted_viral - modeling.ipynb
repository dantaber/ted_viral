{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling \"virality\" of TED Talks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file uses a sequence of ensemble models to systematically shrink the sample of 2,374 TED talks down to a smaller sample of talks that get the most views. Each individual layer, or \"round\", is designed to predict whether a talk is above or below the sample median. In each round, talks that are predicted to be above the median advance to the next round, others are \"eliminated\", a new median is calculated, and the process repeats itself as many times as necessary. Each round tests 6 different classification algorithms and chooses up to 3 that do not overfit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD\n",
    "file = 'df_text'\n",
    "f = open(file,'rb') \n",
    "df = pickle.load(f) \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep only relevant variables\n",
    "df = df[['id', 'speaker', 'URL', 'time', 'technology', 'science', 'global', 'design', 'issues', 'culture', 'business', \n",
    "          'entertainment', 'change', 'art', 'biology', 'innovation', 'education', 'society', 'communication', \n",
    "          'politics', 'future', 'music', 'log_views', 'laughter_n', 'applause_n', 'questions_n', 'stories_n', \n",
    "          'talking_speed', 'laughter_speed', 'sentence_length', 'year', 'polarity', \n",
    "          'sentiment_range', 'sentiment_std', 'cluster_num0', 'cluster_num1', 'cluster_num2',  'cluster_num3', \n",
    "         'cluster_num4', 'cluster_num5', 'Spring', 'Summer', 'Winter', 'exclamation_n', \n",
    "         'he', 'she', 'he_she', 'self', 'we', 'you', 'filler', \n",
    "         'adjectives_percent', 'verbs_percent', 'nouns_percent', 'views']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.views.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('views', ascending = False)[['speaker', 'views']].head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 1% will be used as a definition of \"viral,\" although the threshold doesn't affect the modeling process described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['true_viral'] = df.log_views.apply(lambda x: 1 if x>16.36 else 0)\n",
    "print(df.true_viral.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.true_viral == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore how characteristics of top 1% differ from the remaining 99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('true_viral')['technology', 'science', 'global', 'design', 'issues', 'culture', 'business', \n",
    "                    'entertainment', 'change', 'art', 'biology', 'innovation', 'education', 'society'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('true_viral')['communication', 'politics', 'future', 'music', 'cluster_num0', 'cluster_num1', 'cluster_num2', \n",
    "          'cluster_num3', 'cluster_num4', 'cluster_num5'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('true_viral')['laughter_n', 'applause_n', 'questions_n', 'stories_n', 'exclamation_n', \n",
    "                         'talking_speed', 'laughter_speed', 'sentence_length', 'polarity', 'sentiment_range', \n",
    "                         'sentiment_std'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('true_viral')['he', 'she', 'he_she', 'self', 'you', 'we', 'filler', 'adjectives_percent', 'verbs_percent', 'nouns_percent' ,\n",
    "                         'Spring', 'Summer', 'Winter', 'year'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop variables that are never present in viral talks - art, music, and future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(['art', 'music', 'future'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['technology', 'science', 'global', 'design', 'issues', 'culture', 'business', 'entertainment', \n",
    "            'change', 'biology', 'innovation', 'education', 'society', 'communication', 'politics', \n",
    "            'cluster_num1', 'cluster_num2', 'cluster_num3', 'cluster_num4', \n",
    "            'cluster_num5', 'laughter_n', 'applause_n', 'questions_n', 'stories_n', 'exclamation_n', \n",
    "            'sentence_length', 'polarity', 'sentiment_range', 'sentiment_std', \n",
    "            'he_she', 'self', 'you', 'we', 'filler', 'adjectives_percent', 'verbs_percent', 'nouns_percent' ,\n",
    "            'Spring', 'Summer', 'Winter', 'year']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate definition of viral\n",
    "# NOTE: \"Viral\" is used loosely to represent above or below the median. \n",
    "def viral(data):\n",
    "    views_median = data.log_views.median()\n",
    "    print(views_median)\n",
    "    data['viral'] = data.log_views.apply(lambda x: 1 if x>views_median else 0)\n",
    "    print(data.viral.value_counts())\n",
    "    \n",
    "def new_viral(data):\n",
    "    global new_df\n",
    "    new_df = data[data.y_pred == 1]\n",
    "    viral(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EDA done at each stage to exmaine whether features should be dropped\n",
    "def viral_corr(var, data):\n",
    "    midpoint = len(features) // 2\n",
    "    print(data.groupby(var)[features[:midpoint]].mean())\n",
    "    print(data.groupby(var)[features[midpoint:]].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to use in general classifier\n",
    "def plot_confusion_matrix(cm, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General classifer function\n",
    "def train_score(classifier, x, y, test_size):\n",
    "    mm = MinMaxScaler()\n",
    "    xtrain, xtest, ytrain, ytest = cross_validation.train_test_split(x, y, test_size=test_size, random_state=1234)\n",
    "    xtrain = mm.fit_transform(xtrain)\n",
    "    xtest = mm.transform(xtest)\n",
    "    ytrain = np.ravel(ytrain)    \n",
    "    clf = classifier.fit(xtrain, ytrain)    \n",
    "    \n",
    "    # score the model (accuracy)   \n",
    "    train_acc = clf.score(xtrain, ytrain)\n",
    "    test_acc = clf.score(xtest, ytest)\n",
    "    \n",
    "    print(\"Training Data Accuracy: %0.2f\" %(train_acc))\n",
    "    print(\"Test Data Accuracy:     %0.2f\" %(test_acc))\n",
    "    \n",
    "    # create a confusion matrix\n",
    "    y_true = ytest\n",
    "    y_pred = clf.predict(xtest)   \n",
    "    conf = confusion_matrix(y_true, y_pred)\n",
    "    print ('\\n')\n",
    "    print(conf)\n",
    "\n",
    "    print ('\\n')\n",
    "    print (\"Precision:              %0.2f\" %(conf[0, 0] / (conf[0, 0] + conf[1, 0])))\n",
    "    print (\"Recall:                 %0.2f\"% (conf[0, 0] / (conf[0, 0] + conf[0, 1])))\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=None)\n",
    "\n",
    "    # plot the confusion matrix    \n",
    "    print ('\\n')\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm)\n",
    "    \n",
    "    # ROC curve\n",
    "    y_score = clf.predict_proba(xtest)[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(ytest, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    print('AUC: ', roc_auc)    \n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot([0,1],[0,1]) # this is our baseline\n",
    "    plt.plot(fpr, tpr) # this is our ROC curve\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensemble (data, classifier1, classifier2, classifier3, threshold):\n",
    "    models = {'gbt': model_gbt,\n",
    "              'ada': model_ada,\n",
    "              'rf': model_rf,\n",
    "              'ext': model_ext,\n",
    "              'lr': rfe_lr,\n",
    "              'bay': rfe_Bayes}\n",
    "    \n",
    "    abbrev1 = 'y_score_' + classifier1\n",
    "    model = models[classifier1]\n",
    "    model.fit(X,y)\n",
    "    data[abbrev1] = model.predict_proba(X)[:,1]\n",
    "\n",
    "    abbrev2 = 'y_score_' + classifier2\n",
    "    model = models[classifier2]\n",
    "    model.fit(X,y)\n",
    "    data[abbrev2] = model.predict_proba(X)[:,1]\n",
    "    \n",
    "    abbrev3 = 'y_score_' + classifier3\n",
    "    model = models[classifier3]\n",
    "    model.fit(X,y)\n",
    "    data[abbrev3] = model.predict_proba(X)[:,1]\n",
    "    \n",
    "    data['y_pred'] = 0\n",
    "    data.loc[(data[abbrev1] >= threshold) | (data[abbrev2] >= threshold) | (data[abbrev3] >= threshold), 'y_pred'] = 1\n",
    "    \n",
    "    conf_viral = confusion_matrix(data['viral'], data['y_pred'])\n",
    "    conf_true = confusion_matrix(data['true_viral'], data['y_pred'])\n",
    "    for x in [conf_viral, conf_true]:\n",
    "        print (x)\n",
    "        print (\"Precision:              %0.2f\" %(x[1, 1] / (x[1, 1] + x[0, 1])))\n",
    "        print (\"Recall:                 %0.2f\"% (x[1, 1] / (x[1, 1] + x[1, 0])))\n",
    "        print (\"Accuracy:               %0.2f\"% ((x[1, 1] + x[0, 0]) / (x[0, 0] + x[1, 1] + x[1, 0] + x[0, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.preprocessing import binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viral(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viral_corr('viral', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df['viral']\n",
    "X = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression()\n",
    "\n",
    "rfe_lr = RFE(model_lr, 41)\n",
    "train_score(rfe_lr, X, y, 0.2)\n",
    "\n",
    "print (\"Features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rfe_lr.ranking_), features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Bayes = naive_bayes.BernoulliNB()\n",
    "rfe_Bayes = RFE(model_Bayes, 41)\n",
    "train_score(rfe_Bayes, X, y, 0.2)\n",
    "\n",
    "print (\"Features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rfe_Bayes.ranking_), features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400], 'n_jobs': [-1]}\n",
    "model_rf = GridSearchCV(RandomForestClassifier(max_features = 30, max_depth = 3, random_state = 1234), \n",
    "                        param_grid=param_grid, \n",
    "                        cv=10, \n",
    "                        scoring='recall')\n",
    "train_score(model_rf, X, y, 0.15)\n",
    "print (model_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(max_features = 30, max_depth = 3, random_state = 1234, \n",
    "                        n_estimators = 100,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.15)\n",
    "\n",
    "model_rf.fit(xtrain, ytrain)\n",
    "\n",
    "importances = model_rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in indices[:10]:\n",
    "    print (features[i], round(importances[i], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_ext = GridSearchCV(ExtraTreesClassifier(max_features = 30, max_depth = 3, random_state = 1234), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_ext, X, y, 0.2)\n",
    "print (model_ext.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ext = ExtraTreesClassifier(max_features = 30, max_depth = 3, random_state = 1234, \n",
    "                        n_estimators = 300,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model_ext.fit(xtrain, ytrain)\n",
    "\n",
    "importances = model_ext.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in indices[:10]:\n",
    "    print (features[i], round(importances[i], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_gbt = GridSearchCV(GradientBoostingClassifier(max_features = 15, max_depth = 2, random_state = 1234), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_gbt, X, y, 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_ada = GridSearchCV(AdaBoostClassifier(random_state = 1234), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_ada, X, y, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble(df, 'rf', 'bay', 'lr', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out those who were missed\n",
    "df.loc[(df['true_viral'] == 1.0) & (df['y_pred'] == 0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y_score_mean'] = (df['y_score_ext'] + df['y_score_lr'] + df['y_score_bay']) / 3\n",
    "top_percent = df[['speaker', 'y_score_mean', 'true_viral']].sort_values('y_score_mean', ascending = False).head(24)\n",
    "top_percent.true_viral.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round1_status = df[['id', 'speaker', 'true_viral', 'y_pred', 'views', 'log_views']]\n",
    "round1_status.rename(columns={'y_pred': 'round1_pred'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "round1 = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 2: Repeat on those predicted to be \"viral\" in Round 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each round follows the same sequence of algorithms, using the sample of talks remaining from the previous round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_viral(round1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = new_df['viral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viral_corr('viral', new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in ['politics']:\n",
    "    if i in features:\n",
    "        features.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df[features]\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression()\n",
    "rfe_lr = RFE(model_lr, 28)\n",
    "train_score(rfe_lr, X, y, 0.20)\n",
    "\n",
    "print (\"Features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rfe_lr.ranking_), features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Bayes = naive_bayes.BernoulliNB()\n",
    "rfe_Bayes = RFE(model_Bayes, 20)\n",
    "train_score(rfe_Bayes, X, y, 0.2)\n",
    "\n",
    "print (\"Features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rfe_Bayes.ranking_), features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400], 'n_jobs': [-1]}\n",
    "model_rf = GridSearchCV(RandomForestClassifier(max_features = 25, max_depth = 2, random_state = 1234), \n",
    "                        param_grid=param_grid, \n",
    "                        cv=10, \n",
    "                        scoring='recall')\n",
    "train_score(model_rf, X, y, 0.2)\n",
    "print (model_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(max_features = 25, max_depth = 2, random_state = 1234, \n",
    "                        n_estimators = 200,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.15)\n",
    "\n",
    "model_rf.fit(xtrain, ytrain)\n",
    "\n",
    "importances = model_rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in indices[:10]:\n",
    "    print (features[i], round(importances[i], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_ext = GridSearchCV(ExtraTreesClassifier(max_features = 18, max_depth = 2, random_state = 1234), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_ext, X, y, 0.15)\n",
    "print (model_ext.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ext = ExtraTreesClassifier(max_features = 18, max_depth = 2, random_state = 1234, \n",
    "                        n_estimators = 100,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.15)\n",
    "\n",
    "model_ext.fit(xtrain, ytrain)\n",
    "\n",
    "importances = model_ext.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in indices[:10]:\n",
    "    print (features[i], round(importances[i], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_gbt = GridSearchCV(GradientBoostingClassifier(max_features = 18, max_depth = 2, random_state = 1234), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_gbt, X, y, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_ada = GridSearchCV(AdaBoostClassifier(random_state = 1234), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_ada, X, y, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble(new_df, 'ext', 'lr', 'rf', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.loc[(new_df['true_viral'] == 1.0) & (new_df['y_pred'] == 0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['y_score_mean'] = (new_df['y_score_ext'] + new_df['y_score_bay'] ) / 2\n",
    "top_percent = new_df[['speaker', 'y_score_mean', 'true_viral']].sort_values('y_score_mean', ascending = False).head(24)\n",
    "top_percent.true_viral.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round2_status = new_df[['id', 'y_pred']]\n",
    "round2_status.rename(columns={'y_pred': 'round2_pred'}, inplace=True)\n",
    "\n",
    "rounds1_2 = pd.merge(round1_status, round2_status, on='id', how='outer').fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "round2 = new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_viral(round2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = new_df['viral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viral_corr('viral', new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in ['change']:\n",
    "    features.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df[features]\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression()\n",
    "rfe_lr = RFE(model_lr, 25)\n",
    "train_score(rfe_lr, X, y, 0.2)\n",
    "\n",
    "print (\"Features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rfe_lr.ranking_), features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Bayes = naive_bayes.BernoulliNB()\n",
    "rfe_Bayes = RFE(model_Bayes, 30)\n",
    "train_score(rfe_Bayes, X, y, 0.2)\n",
    "\n",
    "print (\"Features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rfe_Bayes.ranking_), features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400], 'n_jobs': [-1]}\n",
    "model_rf = GridSearchCV(RandomForestClassifier(max_features = 30, max_depth = 1, random_state = 1234), \n",
    "                        param_grid=param_grid, \n",
    "                        cv=10, \n",
    "                        scoring='recall')\n",
    "train_score(model_rf, X, y, 0.3)\n",
    "print(model_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(max_features = 30, max_depth = 1, random_state = 1234, \n",
    "                        n_estimators = 200,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state = 1234)\n",
    "\n",
    "model_rf.fit(xtrain, ytrain)\n",
    "\n",
    "importances = model_rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in indices[:10]:\n",
    "    print (features[i], round(importances[i], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_ext = GridSearchCV(ExtraTreesClassifier(max_features = 12, max_depth = 2, random_state = 1234), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_ext, X, y, 0.3)\n",
    "print(model_ext.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ext = ExtraTreesClassifier(max_features = 12, max_depth = 2, random_state = 1234, \n",
    "                        n_estimators = 100,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "model_ext.fit(xtrain, ytrain)\n",
    "\n",
    "importances = model_ext.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in indices[:10]:\n",
    "    print (features[i], round(importances[i], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_gbt = GridSearchCV(GradientBoostingClassifier(max_features = 20, max_depth = 1, random_state = 1234), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_gbt, X, y, 0.3)\n",
    "print(model_gbt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gbt = GradientBoostingClassifier(max_features = 20, max_depth = 1, random_state = 1234, \n",
    "                        n_estimators = 200)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state = 1234)\n",
    "\n",
    "model_gbt.fit(xtrain, ytrain)\n",
    "\n",
    "importances = model_gbt.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in indices[:10]:\n",
    "    print (features[i], round(importances[i], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_ada = GridSearchCV(AdaBoostClassifier(random_state = 1234), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_ada, X, y, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble(new_df, 'gbt', 'ext', 'lr', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['y_score_mean'] = (new_df['y_score_rf'])\n",
    "top_percent = new_df[['speaker', 'y_score_mean', 'true_viral']].sort_values('y_score_mean', ascending = False).head(24)\n",
    "top_percent.true_viral.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round3_status = new_df[['id', 'y_pred']]\n",
    "round3_status.rename(columns={'y_pred': 'round3_pred'}, inplace=True)\n",
    "\n",
    "rounds123 = pd.merge(rounds1_2, round3_status, on='id', how='outer').fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "round3 = new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Round 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_viral(round3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = new_df['viral']\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viral_corr('viral', new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in ['design']:\n",
    "    if i in features:\n",
    "        features.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = new_df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression()\n",
    "rfe = RFE(model_lr, 25)\n",
    "train_score(rfe, X, y, 0.2)\n",
    "\n",
    "print (\"Features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Bayes = naive_bayes.BernoulliNB()\n",
    "rfe = RFE(model_Bayes, 15)\n",
    "train_score(rfe, X, y, 0.2)\n",
    "\n",
    "print (\"Features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400], 'n_jobs': [-1]}\n",
    "model_rf = GridSearchCV(RandomForestClassifier(max_features = 30, max_depth = 1, random_state = 1234), \n",
    "                        param_grid=param_grid, \n",
    "                        cv=10, \n",
    "                        scoring='recall')\n",
    "train_score(model_rf, X, y, 0.30)\n",
    "print(model_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(max_features = 30, max_depth = 1, random_state = 1234, \n",
    "                        n_estimators = 300,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state = 1234)\n",
    "\n",
    "model_rf.fit(xtrain, ytrain)\n",
    "\n",
    "importances = model_rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in indices[:10]:\n",
    "    print (features[i], round(importances[i], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_ext = GridSearchCV(ExtraTreesClassifier(max_features = 20, max_depth = 3, random_state = 1234), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_ext, X, y, 0.3)\n",
    "print(model_ext.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ext = ExtraTreesClassifier(max_features = 20, max_depth = 3, random_state = 1234, \n",
    "                        n_estimators = 100,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state = 1234)\n",
    "\n",
    "model_ext.fit(xtrain, ytrain)\n",
    "\n",
    "importances = model_ext.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in indices[:10]:\n",
    "    print (features[i], round(importances[i], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_gbt = GridSearchCV(GradientBoostingClassifier(max_features = 25, max_depth = 1, random_state = 1234), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_gbt, X, y, 0.3)\n",
    "print(model_gbt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gbt = GradientBoostingClassifier(max_features = 25, max_depth = 1, random_state = 1234, \n",
    "                        n_estimators = 100)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state = 1234)\n",
    "\n",
    "model_gbt.fit(xtrain, ytrain)\n",
    "\n",
    "importances = model_gbt.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in indices[:10]:\n",
    "    print (features[i], round(importances[i], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_ada = GridSearchCV(AdaBoostClassifier(random_state = 1234), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_ada, X, y, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble(new_df, 'lr', 'gbt', 'gbt', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.loc[(new_df['true_viral'] == 1.0) & (new_df['y_pred'] == 0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['y_score_mean'] = (new_df['y_score_ext'] + new_df['y_score_lr'] ) / 2\n",
    "top_percent = new_df[['speaker', 'y_score_mean', 'true_viral']].sort_values('y_score_mean', ascending = False).head(24)\n",
    "top_percent.true_viral.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round4_status = new_df[['id', 'y_pred']]\n",
    "round4_status.rename(columns={'y_pred': 'round4_pred'}, inplace=True)\n",
    "\n",
    "rounds1234 = pd.merge(rounds123, round4_status, on='id', how='outer').fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "round4 = new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Round 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_viral(round4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = new_df['viral']\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viral_corr('viral', new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in ['innovation', 'society']:\n",
    "    if i in features:\n",
    "        features.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df[features]\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression()\n",
    "rfe = RFE(model_lr, 10)\n",
    "train_score(rfe, X, y, 0.2)\n",
    "\n",
    "print (\"Features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Bayes = naive_bayes.BernoulliNB()\n",
    "rfe = RFE(model_Bayes, 20)\n",
    "train_score(rfe, X, y, 0.2)\n",
    "\n",
    "print (\"Features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400], 'n_jobs': [-1]}\n",
    "model_rf = GridSearchCV(RandomForestClassifier(max_features = 25, max_depth = 1, random_state = 1234), \n",
    "                        param_grid=param_grid, \n",
    "                        cv=10, \n",
    "                        scoring='recall')\n",
    "train_score(model_rf, X, y, 0.3)\n",
    "print(model_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_ext = GridSearchCV(ExtraTreesClassifier(max_features = 30, max_depth = 1, random_state = 1234), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_ext, X, y, 0.2)\n",
    "print(model_ext.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ext = ExtraTreesClassifier(max_features = 30, max_depth = 1, random_state = 1234, \n",
    "                        n_estimators = 400,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state = 1234)\n",
    "\n",
    "model_ext.fit(xtrain, ytrain)\n",
    "\n",
    "importances = model_ext.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in indices[:10]:\n",
    "    print (features[i], round(importances[i], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_gbt = GridSearchCV(GradientBoostingClassifier(max_features = 10, max_depth = 1, random_state = 1234), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_gbt, X, y, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_ada = GridSearchCV(AdaBoostClassifier(random_state = 1234), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_ada, X, y, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble(new_df, 'bay', 'ext', 'bay', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.loc[(new_df['true_viral'] == 1.0) & (new_df['y_pred'] == 0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['y_score_mean'] = (new_df['y_score_ext'] + new_df['y_score_lr'] + new_df['y_score_rf'] ) / 3\n",
    "top_percent = new_df[['speaker', 'y_score_mean', 'true_viral']].sort_values('y_score_mean', ascending = False).head(24)\n",
    "top_percent.true_viral.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round5_status = new_df[['id', 'y_pred']]\n",
    "round5_status.rename(columns={'y_pred': 'round5_pred'}, inplace=True)\n",
    "\n",
    "rounds12345 = pd.merge(rounds1234, round5_status, on='id', how='outer').fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "round5 = new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Round 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_viral(round5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = new_df['viral']\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "viral_corr('viral', new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in ['society', 'communication']:\n",
    "    if i in features:\n",
    "        features.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df[features]\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression()\n",
    "rfe = RFE(model_lr, 12)\n",
    "train_score(rfe, X, y, 0.2)\n",
    "\n",
    "print (\"Features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Bayes = naive_bayes.BernoulliNB()\n",
    "rfe = RFE(model_Bayes, 12)\n",
    "train_score(rfe, X, y, 0.2)\n",
    "\n",
    "print (\"Features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400], 'n_jobs': [-1]}\n",
    "model_rf = GridSearchCV(RandomForestClassifier(max_features = 30, max_depth = 1, random_state = 1234), \n",
    "                        param_grid=param_grid, \n",
    "                        cv=10, \n",
    "                        scoring='recall')\n",
    "train_score(model_rf, X, y, 0.2)\n",
    "print(model_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(max_features = 30, max_depth = 1, random_state = 1234, \n",
    "                        n_estimators = 100,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state = 1234)\n",
    "\n",
    "model_rf.fit(xtrain, ytrain)\n",
    "\n",
    "importances = model_rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in indices[:20]:\n",
    "    print (features[i], round(importances[i], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_gbt = GridSearchCV(GradientBoostingClassifier(max_features = 10, max_depth = 1, random_state = 1234 ), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_gbt, X, y, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_ada = GridSearchCV(AdaBoostClassifier(random_state = 1234), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_ada, X, y, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 200, 300, 400]}\n",
    "model_ext = GridSearchCV(ExtraTreesClassifier(max_features = 25, max_depth = 1, random_state = 1234), \n",
    "                         param_grid=param_grid, \n",
    "                         cv=10, \n",
    "                         scoring='recall')\n",
    "train_score(model_ext, X, y, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ensemble(new_df, 'rf', 'rf', 'rf', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.loc[(new_df['true_viral'] == 1.0) & (new_df['y_pred'] == 0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['y_score_mean'] = (new_df['y_score_rf'] + new_df['y_score_rf'] + new_df['y_score_rf'] ) / 3\n",
    "top_percent = new_df[['speaker', 'y_score_mean', 'true_viral']].sort_values('y_score_mean', ascending = False).head(24)\n",
    "top_percent.true_viral.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round6_status = new_df[['id', 'y_pred']]\n",
    "round6_status.rename(columns={'y_pred': 'round6_pred'}, inplace=True)\n",
    "\n",
    "rounds123456 = pd.merge(rounds12345, round6_status, on='id', how='outer').fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(2, 7):\n",
    "    name = 'round' + str(i) + '_pred'\n",
    "    rounds123456[name] = rounds123456[name].map(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rounds123456.to_csv('round-by-round status.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
